#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cublas_v2.h>
#include <cmath>
#include <iostream>

#define HIDDEN_SIZE 4096
#define INTERMEDIATE_SIZE 12288

#define CHECK_CUDA(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " \
                    << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while(0)

#define CHECK_CUBLAS(call) \
    do { \
        cublasStatus_t status = call; \
        if (status != CUBLAS_STATUS_SUCCESS) { \
            std::cerr << "cuBLAS error at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(1); \
        } \
    } while(0)

// ====================================================================================
// GELU Activation (using inverse tanh)
// ====================================================================================

__device__ __forceinline__ float gelu(float x) {
    const float sqrt_2_over_pi = 0.7978845608f;
    const float coeff = 0.044715f;
    float x_cubed = x * x * x;
    float inner = sqrt_2_over_pi * (x + coeff * x_cubed);
    return 0.5f * x * (1.0f + tanhf(inner));
}

// ====================================================================================
// My Kernel: Thread-Tiling for Better Arithmetic Intensity
// ====================================================================================

// Increases arithmetic intensity (FLOPs per byte loaded)

template<int TILE_SIZE = 4>
__global__ void geglu_tiled_kernel(
    const float* __restrict__ x,
    const float* __restrict__ Wu,
    const float* __restrict__ Wv,
    float* __restrict__ intermediate,
    int B
) {
    // Each thread computes TILE_SIZE output elements
    const int out_start = (blockIdx.x * blockDim.x + threadIdx.x) * TILE_SIZE;
    const int batch_idx = blockIdx.y;
    
    if (out_start >= INTERMEDIATE_SIZE || batch_idx >= B) return;
    
    // Thread-local accumulators
    float u_sums[TILE_SIZE] = {0.0f};
    float v_sums[TILE_SIZE] = {0.0f};
    
    // Load input once, reuse across TILE_SIZE outputs
    const float* x_row = x + batch_idx * HIDDEN_SIZE;
    
    // Vectorized loads
    const float4* x_vec = reinterpret_cast<const float4*>(x_row);
    const int num_vec = HIDDEN_SIZE / 4;
    
    // Compute all TILE_SIZE outputs in parallel
    for (int i = 0; i < num_vec; i++) {
        float4 x_val = x_vec[i];
        
        #pragma unroll
        for (int t = 0; t < TILE_SIZE; t++) {
            int out_idx = out_start + t;
            if (out_idx < INTERMEDIATE_SIZE) {
                const float4* Wu_vec = reinterpret_cast<const float4*>(Wu + out_idx * HIDDEN_SIZE);
                const float4* Wv_vec = reinterpret_cast<const float4*>(Wv + out_idx * HIDDEN_SIZE);
                
                float4 wu_val = Wu_vec[i];
                float4 wv_val = Wv_vec[i];
                
                u_sums[t] += x_val.x * wu_val.x + x_val.y * wu_val.y +
                             x_val.z * wu_val.z + x_val.w * wu_val.w;
                v_sums[t] += x_val.x * wv_val.x + x_val.y * wv_val.y +
                             x_val.z * wv_val.z + x_val.w * wv_val.w;
            }
        }
    }
    
    // Write results
    #pragma unroll
    for (int t = 0; t < TILE_SIZE; t++) {
        int out_idx = out_start + t;
        if (out_idx < INTERMEDIATE_SIZE) {
            intermediate[batch_idx * INTERMEDIATE_SIZE + out_idx] = gelu(u_sums[t]) * v_sums[t];
        }
    }
}

void geglu_ffn_tiled(
    cublasHandle_t handle,
    const float* x,
    const float* Wu,
    const float* Wv,
    const float* Wo,
    float* intermediate,
    float* output,
    int B
) {
    float alpha = 1.0f, beta = 0.0f;
    
    const int TILE_SIZE = 4;
    const int THREADS = 256;
    const int TILES_PER_BLOCK = THREADS;
    const int outputs_per_block = TILES_PER_BLOCK * TILE_SIZE;
    
    dim3 block(THREADS);
    dim3 grid((INTERMEDIATE_SIZE + outputs_per_block - 1) / outputs_per_block, B);
    
    geglu_tiled_kernel<TILE_SIZE><<<grid, block>>>(x, Wu, Wv, intermediate, B);
    CHECK_CUDA(cudaGetLastError());
    
    // Down projection
    CHECK_CUBLAS(cublasSgemm(
        handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        HIDDEN_SIZE, B, INTERMEDIATE_SIZE,
        &alpha,
        Wo, HIDDEN_SIZE,
        intermediate, INTERMEDIATE_SIZE,
        &beta,
        output, HIDDEN_SIZE
    ));
}
