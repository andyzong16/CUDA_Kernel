#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cublas_v2.h>
#include <cmath>
#include <iostream>

#define HIDDEN_SIZE 4096
#define INTERMEDIATE_SIZE 12288

#define CHECK_CUDA(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " \
                    << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while(0)

#define CHECK_CUBLAS(call) \
    do { \
        cublasStatus_t status = call; \
        if (status != CUBLAS_STATUS_SUCCESS) { \
            std::cerr << "cuBLAS error at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(1); \
        } \
    } while(0)

// ====================================================================================
// GELU Activation (using inverse tanh)
// ====================================================================================

__device__ __forceinline__ float gelu(float x) {
    const float k = 0.7978845608f;  // sqrt(2/pi)
    return 0.5f * x * (1.0f + tanhf(k * (x + 0.044715f * x * x * x)));
}


// ====================================================================================
// GEGLU elementwise kernel
// ====================================================================================

__global__ void geglu_kernel(
    const float* __restrict__ U,
    const float* __restrict__ V,
    float* __restrict__ Y,
    int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= n) return;

    float u = U[idx];
    float v = V[idx];

    Y[idx] = gelu(u) * v;
}


// ====================================================================================
// Fast GEGLU FFN (cuBLAS + tiny kernel)
// ====================================================================================

void geglu_ffn(
    cublasHandle_t handle,
    const float* x,    
    const float* Wu,    
    const float* Wv,    
    const float* Wo,    
    float* U,          
    float* V,           
    float* intermediate,
    float* output,      
    int B
) {
    const float alpha = 1.0f;
    const float beta  = 0.0f;


    CHECK_CUBLAS(cublasSgemm(
        handle,
        CUBLAS_OP_T, CUBLAS_OP_N,
        INTERMEDIATE_SIZE, B, HIDDEN_SIZE,
        &alpha,
        Wu, HIDDEN_SIZE,
        x,  HIDDEN_SIZE,
        &beta,
        U,  INTERMEDIATE_SIZE
    ));


    CHECK_CUBLAS(cublasSgemm(
        handle,
        CUBLAS_OP_T, CUBLAS_OP_N,
        INTERMEDIATE_SIZE, B, HIDDEN_SIZE,
        &alpha,
        Wv, HIDDEN_SIZE,
        x,  HIDDEN_SIZE,
        &beta,
        V,  INTERMEDIATE_SIZE
    ));


    int n = B * INTERMEDIATE_SIZE;
    int threads = 256;
    int blocks = (n + threads - 1) / threads;

    geglu_kernel<<<blocks, threads>>>(U, V, intermediate, n);
    CHECK_CUDA(cudaGetLastError());


    CHECK_CUBLAS(cublasSgemm(
        handle,
        CUBLAS_OP_T, CUBLAS_OP_N,
        HIDDEN_SIZE, B, INTERMEDIATE_SIZE,
        &alpha,
        Wo, INTERMEDIATE_SIZE,
        intermediate, INTERMEDIATE_SIZE,
        &beta,
        output, HIDDEN_SIZE
    ));
}
