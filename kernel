#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cublas_v2.h>
#include <cmath>
#include <iostream>
#include <map>

#define HIDDEN_SIZE 4096
#define INTERMEDIATE_SIZE 12288

// ====================================================================================
// Error checking macros
// ====================================================================================
#define CHECK_CUDA(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " \
                      << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while (0)

#define CHECK_CUBLAS(call) \
    do { \
        cublasStatus_t err = call; \
        if (err != CUBLAS_STATUS_SUCCESS) { \
            std::cerr << "cuBLAS error at " << __FILE__ << ":" << __LINE__ << " - " \
                      << "status " << err << std::endl; \
            exit(1); \
        } \
    } while (0)

// ====================================================================================
// Fast GELU Device Function
// ====================================================================================
__device__ __forceinline__ float fast_gelu(float x) {
    const float k = 0.79788456f;
    const float c = 0.044715f;
    return 0.5f * x * (1.0f + tanhf(k * (x + c * x * x * x)));
}

// ====================================================================================
// Fused KERNEL for GEGLU Activation
// ====================================================================================
__global__ void geglu_kernel_fused(const half2* __restrict__ U, half2* __restrict__ V, half2* __restrict__ Y, int n_half2) {
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 2;
    if (idx >= n_half2) return;

    half2 uh = __ldg(&U[idx]);
    half2 vh = __ldg(&V[idx]);

    float2 u = __half22float2(uh);
    float2 v = __half22float2(vh);

    float gx = fast_gelu(u.x) * v.x;
    float gy = fast_gelu(u.y) * v.y;

    Y[idx] = __floats2half2_rn(gx, gy);
}

// ====================================================================================
// FFN Logic with GEGLU Activation
// ====================================================================================
void geglu_ffn(
    cublasHandle_t handle,
    const __half* x,      
    const __half* Wu,     
    const __half* Wv,     
    const __half* Wo,     
    __half* U,            
    __half* V,            
    __half* intermediate, 
    __half* output,       
    int B
) {
    const float alpha = 1.0f;
    const float beta  = 0.0f;

    CHECK_CUBLAS(cublasGemmEx(
        handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        INTERMEDIATE_SIZE, B, HIDDEN_SIZE,
        &alpha,
        Wu, CUDA_R_16F, INTERMEDIATE_SIZE,
        x,  CUDA_R_16F, HIDDEN_SIZE,
        &beta,
        U,  CUDA_R_16F, INTERMEDIATE_SIZE,
        CUBLAS_COMPUTE_32F,
        CUBLAS_GEMM_DEFAULT_TENSOR_OP));

    CHECK_CUBLAS(cublasGemmEx(
        handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        INTERMEDIATE_SIZE, B, HIDDEN_SIZE,
        &alpha,
        Wv, CUDA_R_16F, INTERMEDIATE_SIZE,
        x,  CUDA_R_16F, HIDDEN_SIZE,
        &beta,
        V,  CUDA_R_16F, INTERMEDIATE_SIZE,
        CUBLAS_COMPUTE_32F,
        CUBLAS_GEMM_DEFAULT_TENSOR_OP));

    int total = B * INTERMEDIATE_SIZE;
    int n_half2 = total / 2;

    int threads = 128;
    int blocks  = (n_half2 + threads - 1) / threads;

    geglu_kernel_fused<<<blocks, threads>>>(
        (half2*)U,
        (half2*)V,
        (half2*)intermediate,
        n_half2);

    CHECK_CUDA(cudaGetLastError());

    CHECK_CUBLAS(cublasGemmEx(
        handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        HIDDEN_SIZE, B, INTERMEDIATE_SIZE,
        &alpha,
        Wo, CUDA_R_16F, HIDDEN_SIZE,
        intermediate, CUDA_R_16F, INTERMEDIATE_SIZE,
        &beta,
        output, CUDA_R_16F, HIDDEN_SIZE,
        CUBLAS_COMPUTE_32F,
        CUBLAS_GEMM_DEFAULT_TENSOR_OP));
}
